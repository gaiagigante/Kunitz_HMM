{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Selection of the dataset to build the profile HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdb_query = '''{\"query\":{\"type\":\"group\",\"logical_operator\":\"and\",\"nodes\":[{\"type\":\"group\",\"logical_operator\":\"and\",\"nodes\":[{\"type\":\"terminal\",\"service\":\"text\",\"parameters\":{\"attribute\":\"rcsb_polymer_entity_annotation.annotation_id\",\"operator\":\"exact_match\",\"negation\":false,\"value\":\"PF00014\"}},{\"type\":\"terminal\",\"service\":\"text\",\"parameters\":{\"attribute\":\"rcsb_polymer_entity_annotation.type\",\"operator\":\"exact_match\",\"value\":\"Pfam\",\"negation\":false}}],\"label\":\"nested-attribute\"},{\"type\":\"terminal\",\"service\":\"text\",\"parameters\":{\"attribute\":\"rcsb_entry_info.diffrn_resolution_high.value\",\"operator\":\"less_or_equal\",\"negation\":false,\"value\":3}},{\"type\":\"terminal\",\"service\":\"text\",\"parameters\":{\"attribute\":\"entity_poly.rcsb_sample_sequence_length\",\"operator\":\"range\",\"negation\":false,\"value\":{\"from\":50,\"to\":80,\"include_lower\":true,\"include_upper\":true}}},{\"type\":\"terminal\",\"service\":\"text\",\"parameters\":{\"attribute\":\"entity_poly.rcsb_mutation_count\",\"operator\":\"equals\",\"negation\":false,\"value\":0}}],\"label\":\"text\"},\"return_type\":\"polymer_entity\",\"request_options\":{\"group_by_return_type\":\"representatives\",\"group_by\":{\"aggregation_method\":\"sequence_identity\",\"ranking_criteria_type\":{\"sort_by\":\"rcsb_entry_info.resolution_combined\",\"direction\":\"asc\"},\"similarity_cutoff\":50},\"paginate\":{\"start\":0,\"rows\":25},\"results_content_type\":[\"experimental\"],\"sort\":[{\"sort_by\":\"score\",\"direction\":\"desc\"}],\"scoring_strategy\":\"combined\"}}'''\n",
    "req = requests.get('http://search.rcsb.org/rcsbsearch/v2/query?json=%s' % requests.utils.requote_uri(pdb_query))\n",
    "pdb_dict = req.json()\n",
    "\n",
    "list_ids = []\n",
    "for name in pdb_dict['result_set']:\n",
    "    list_ids.append(name['identifier'])\n",
    "\n",
    "with open('input_msa.txt', 'w') as input_msa:  # Input file for PDBeFold\n",
    "    with open ('pdb_bpti.fasta', 'w') as pdb_btpi: # FASTA file with ID and sequence of retrieved proteins\n",
    "        for protein_id in list_ids:\n",
    "            entry_id, chain_id= protein_id.split(\"_\")\n",
    "            req2= requests.get('https://data.rcsb.org/rest/v1/core/polymer_entity/%s/%s' %(entry_id,chain_id))\n",
    "            data = req2.json()\n",
    "            sequence = data['entity_poly']['pdbx_seq_one_letter_code']\n",
    "            chain=data['rcsb_polymer_entity_container_identifiers']['auth_asym_ids'][0]\n",
    "            input_msa.write(entry_id + ':' + chain + '\\n')\n",
    "            pdb_btpi.write ('>' + entry_id + '_' + chain + '\\n' + sequence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multiple Sequence Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSA retrieved from the multiple structural alignment performed with PDBeFold, is saved in the 'kunitz_3d.aln' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse kunitz_3d.aln file to generate a proper input file for HMMER build\n",
    "\n",
    "with open('kunitz_3d.aln') as aln:\n",
    "    with open('clean_kunitz_3d.aln', 'w') as aln2:\n",
    "            for line in aln:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    sections = line.split()\n",
    "                    aln2.write('\\n'+sections[0]+'\\n')\n",
    "                else:\n",
    "                    aln2.write(line.upper()) # write the whole sequence on the same line\n",
    "        \n",
    "with open('clean_kunitz_3d.aln', 'r') as clean_aln:               \n",
    "    with open('cut_kunitz_3d.aln', 'w') as cut_aln:\n",
    "        for line in clean_aln:\n",
    "            line = line.strip()\n",
    "            if line == '\\n': \n",
    "                continue\n",
    "            elif line.startswith('>'):\n",
    "                cut_aln.write('\\n'+line+'\\n')\n",
    "            else:\n",
    "                cut_aln.write(line[19:79]) #select the positions containing the domain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building of the profile HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhmmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmmer build: build the HMM\n",
    "#generation of the alphabet object\n",
    "alphabet = pyhmmer.easel.Alphabet.amino()\n",
    "\n",
    "#loading the alignment and reading it in a digital format\n",
    "with pyhmmer.easel.MSAFile(\"cut_kunitz_3d.aln\", digital=True, alphabet=alphabet) as msa_file:\n",
    "    msa = msa_file.read()\n",
    "\n",
    "msa.name = b\"kunitz_protein_domain\"\n",
    "    \n",
    "#initialize a builder and a background and build an HMM\n",
    "builder = pyhmmer.plan7.Builder(alphabet)\n",
    "background = pyhmmer.plan7.Background(alphabet)\n",
    "hmm, _, _ = builder.build_msa(msa, background)\n",
    "\n",
    "#saving the resulting HMM\n",
    "with open(\"kunitz_3d.hmm\", \"wb\") as output_file:\n",
    "    hmm.write(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Definition of the positive and negative benchmark sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import subprocess\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSITIVE benchmark set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_url = 'https://rest.uniprot.org/uniprotkb/stream?compressed=false&format=fasta&query=%28%28xref%3Apfam-PF00014%29+AND+%28reviewed%3Atrue%29%29'\n",
    "pos_fastas = requests.get(pos_url).text\n",
    "pos_bpti_fasta = re.split(r'\\n(?=>)', pos_fastas) #list of fastas for each entry retrieved\n",
    "#print(len(pos_bpti_fasta)) #391\n",
    "\n",
    "with open('all_bpti.fasta', 'w') as pos_file:\n",
    "    for fasta in pos_bpti_fasta:\n",
    "        fields = fasta.split('\\n')\n",
    "        for line in fields:\n",
    "            if line.startswith('>'):\n",
    "                pos_file.write('\\n'+line+'\\n')\n",
    "            else:\n",
    "                pos_file.write(line)\n",
    "\n",
    "#remove the first empty line from the file\n",
    "with open('all_bpti.fasta', 'r') as pos_file:\n",
    "    lines = pos_file.readlines()\n",
    "with open('all_bpti.fasta', 'w') as right_pos:\n",
    "    for line in lines:\n",
    "        if line.strip() != '':\n",
    "            right_pos.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refinement of the positive benchmark set through BLAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_generation = 'makeblastdb -in pdb_bpti.fasta -dbtype prot'\n",
    "blastp_search = \"blastp -query all_bpti.fasta -db pdb_bpti.fasta -out all_bpti.blast -outfmt 7\"\n",
    "subprocess.run(db_generation, shell=True, check=True)\n",
    "subprocess.run(blastp_search, shell=True, check=True) \n",
    "\n",
    "remove_ids = []\n",
    "with open('all_bpti.blast', 'r') as positives:\n",
    "    for line in positives:\n",
    "        if line.startswith('#'): \n",
    "            continue\n",
    "        else:\n",
    "            sections = line.strip().split('\\t')\n",
    "            acc_ver = sections[0].split('|')[1]\n",
    "            perc_identity = float(sections[2])\n",
    "            aln_length = int(sections[3])\n",
    "            if perc_identity > 95 and aln_length > 50:\n",
    "                remove_ids.append(acc_ver)\n",
    "\n",
    "rem_uniq_ids = list(set(remove_ids)) #list of ids to be removed \n",
    "#print(len(rem_uniq_ids)) #33\n",
    "\n",
    "with open('all_bpti.fasta', 'r') as all_bpti:\n",
    "    with open('all_selected_pos.fasta', 'w') as all_selected: #final file with ids and sequences of selected positive proteins for kunitz domain\n",
    "        selected = all_bpti.readlines()\n",
    "        for line in range(len(selected)):\n",
    "            if '>' in selected[line]:\n",
    "                id = selected[line].split('|')[1]\n",
    "                if id not in rem_uniq_ids:\n",
    "                    all_selected.write(selected[line]+selected[line+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEGATIVE benchmark set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_url = 'https://rest.uniprot.org/uniprotkb/stream?format=fasta&query=%28NOT+%28xref%3Apfam-PF00014%29+AND+%28reviewed%3Atrue%29%29'\n",
    "neg_fastas = requests.get(neg_url).text\n",
    "neg_bpti_fasta = re.split(r'\\n(?=>)', neg_fastas) \n",
    "print(len(neg_bpti_fasta))\n",
    "\n",
    "with open('all_non_bpti.fasta', 'w') as neg_file:\n",
    "    for fasta in neg_bpti_fasta:\n",
    "        fields = fasta.split('\\n')\n",
    "        for line in fields:\n",
    "            if line.startswith('>'):\n",
    "                neg_file.write('\\n'+line+'\\n')\n",
    "            else:\n",
    "                neg_file.write(line)\n",
    "\n",
    "#remove the first empty line from the file\n",
    "with open('all_non_bpti.fasta', 'r') as neg_file:\n",
    "    lines = neg_file.readlines()\n",
    "with open('all_non_bpti.fasta', 'w') as right_neg: #final file with ids and sequences of negative proteins for kunitz domain\n",
    "    for line in lines:\n",
    "        if line.strip() != '':\n",
    "            right_neg.write(line) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Processing of the datasets before applying the HMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the necessary functions to shuffle and split the datasets\n",
    "\n",
    "def shuffle_fasta_file(filename):\n",
    "    '''Takes a file, creates a list of id and sequence pairs and shuffles them'''\n",
    "    with open(filename, 'r') as content:\n",
    "        lines = content.readlines()\n",
    "        pairs = []\n",
    "        for pos in range(0, len(lines), 2):\n",
    "            pairs.append((lines[pos], lines[pos+1]))\n",
    "        random.shuffle(pairs)\n",
    "        return pairs\n",
    "\n",
    "def split_list_into_files (list_name, first_part, second_part):\n",
    "    '''Takes a list and splits its content into two files'''\n",
    "    split_point = int(len(list_name) / 2)\n",
    "    first_half = list_name[:split_point]\n",
    "    second_half = list_name[split_point:]\n",
    "    with open(first_part, 'w') as part1:\n",
    "        for tupla in first_half:\n",
    "            for id_seq in tupla:\n",
    "                if id_seq.endswith('\\n'):\n",
    "                    part1.write(id_seq)\n",
    "                else:\n",
    "                    part1.write(id_seq+'\\n')\n",
    "    with open(second_part, 'w') as part2:\n",
    "        for tupla_2 in second_half:\n",
    "            for id_seq_2 in tupla_2:\n",
    "                if id_seq_2.endswith('\\n'):\n",
    "                    part2.write(id_seq_2)\n",
    "                else:\n",
    "                    part2.write(id_seq_2+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the functions to the positive dataset\n",
    "\n",
    "pos_sequences = shuffle_fasta_file('all_selected_pos.fasta')\n",
    "split_list_into_files(pos_sequences, 'pos1.fasta', 'pos2.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the functions to the negative dataset\n",
    "\n",
    "neg_sequences = shuffle_fasta_file('all_non_bpti.fasta')\n",
    "split_list_into_files(neg_sequences, 'neg1.fasta', 'neg2.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Apply the HMM to each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the functions needed to handle the hmmersearch output\n",
    "\n",
    "def apply_hmm_to_sequence_db(hmm_out_file, db_file):    \n",
    "\n",
    "    '''Takes in input the output of hmmerbuild and the database\n",
    "    with sequences to which apply the hmm, and returns the found hits'''  \n",
    "\n",
    "    with pyhmmer.plan7.HMMFile(hmm_out_file) as hmm_file:\n",
    "        hmm = hmm_file.read()\n",
    "\n",
    "    with pyhmmer.easel.SequenceFile(db_file, digital=True) as seq_file:\n",
    "        sequences = seq_file.read_block()\n",
    "\n",
    "    pipeline = pyhmmer.plan7.Pipeline(alphabet, Z=1, domZ=1, bias_filter=False, F1=1.0, F2=1.0, F3=1.0)\n",
    "    hits = pipeline.search_hmm(hmm, sequences)\n",
    "    return hits\n",
    "\n",
    "def tabular_output_file(hits, filename):\n",
    "\n",
    "    '''Takes in input the hits retrieved from hmmsearch \n",
    "    and creates a tabular format for the hits'''\n",
    "\n",
    "    with open(filename, 'wb') as output_file:\n",
    "        hits.write(output_file, format=\"targets\", header=True)\n",
    "\n",
    "def parse_pos_hits_file(input_file, output_file):\n",
    "\n",
    "    '''Takes in input the tabular file with the hits and generates\n",
    "    an output file with the format: hit_ID   E-value   1'''\n",
    "\n",
    "    with open(input_file, 'r') as pos_hits_file:\n",
    "        with open(output_file, 'w') as pos_parsed_file:\n",
    "            for line in pos_hits_file:\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                line = line.strip()\n",
    "                fields = line.split()\n",
    "                id = fields[0].split('|')[1]\n",
    "                e_value = fields[7]\n",
    "                pos_label = '1'\n",
    "                pos_parsed_file.write(id+' '+e_value+' '+pos_label+'\\n')\n",
    "\n",
    "def parse_neg_hits_file(input_hits_file, input_fasta_file, output_file):\n",
    "\n",
    "    '''Takes in input the tabular file with the hits and generates\n",
    "    an output file with the format: hit_ID   E-value   0'''\n",
    "\n",
    "    tot_ids = set()\n",
    "    with open(input_fasta_file, 'r') as neg_fasta:\n",
    "        for line in neg_fasta:\n",
    "            if line.startswith('>'):\n",
    "                code = line.split('|')[1]\n",
    "                tot_ids.add(code)\n",
    "    hits_ids = set()\n",
    "    with open(input_hits_file, 'r') as neg_hits_file:\n",
    "        with open(output_file, 'w') as neg_parsed_file:\n",
    "            for line in neg_hits_file:\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                line = line.strip()\n",
    "                fields = line.split()\n",
    "                id = fields[0].split('|')[1]\n",
    "                hits_ids.add(id)\n",
    "                e_value = fields[7]\n",
    "                neg_label = '0'\n",
    "                neg_parsed_file.write(id+' '+e_value+' '+neg_label+'\\n')\n",
    "                \n",
    "            diff_ids = tot_ids - hits_ids\n",
    "            for id2 in diff_ids:\n",
    "                neg_parsed_file.write(id2+' '+'10'+' '+'0'+'\\n')\n",
    "\n",
    "def concatenate_files(file_1, file_2, output_file):\n",
    "    '''Takes two input files and concatenates them'''\n",
    "    with open(file_1, 'r') as file1:\n",
    "        content_1 = file1.read()\n",
    "    with open(file_2, 'r') as file2:\n",
    "        content_2 = file2.read()\n",
    "    with open(output_file, 'w') as output:\n",
    "        output.write(content_1 + content_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying the functions to the 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos1.fasta\n",
    "hits_1 = apply_hmm_to_sequence_db('kunitz_3d.hmm', 'pos1.fasta')\n",
    "tabular_output_file(hits_1, 'pos1.out')\n",
    "parse_pos_hits_file('pos1.out', 'pos1.txt')\n",
    "\n",
    "# pos2.fasta\n",
    "hits_2 = apply_hmm_to_sequence_db('kunitz_3d.hmm', 'pos2.fasta')\n",
    "tabular_output_file(hits_2, 'pos2.out')\n",
    "parse_pos_hits_file('pos2.out', 'pos2.txt')\n",
    "\n",
    "# neg1.fasta\n",
    "hits_3 = apply_hmm_to_sequence_db('kunitz_3d.hmm', 'neg1.fasta')\n",
    "tabular_output_file(hits_3, 'neg1.out')\n",
    "parse_neg_hits_file('neg1.out', 'neg1.fasta', 'neg1.txt')\n",
    "\n",
    "# neg2.fasta\n",
    "hits_4 = apply_hmm_to_sequence_db('kunitz_3d.hmm', 'neg2.fasta')\n",
    "tabular_output_file(hits_4, 'neg2.out')\n",
    "parse_neg_hits_file('neg2.out', 'neg2.fasta', 'neg2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenate the final .txt files (one with positive proteins and the other with negative ones) to obtain the final sets and finally merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_files('pos1.txt', 'neg1.txt', 'set1.txt')\n",
    "concatenate_files('pos2.txt', 'neg2.txt', 'set2.txt')\n",
    "\n",
    "#generate a unique set\n",
    "concatenate_files('set1.txt', 'set2.txt', 'whole_set.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Divide the whole dataset and perform the 2-folds cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divide the dataset in 70% (train set) and 30% (teste set) and then generate the 2 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(set, ratio):\n",
    "\n",
    "    '''Takes a file and a ratio and divides the elements \n",
    "    of the file based on the ratio. '''\n",
    "\n",
    "    poss = []\n",
    "    negs = []\n",
    "    for el in set:\n",
    "        if el[-2] == '1':\n",
    "            poss.append(el)\n",
    "        elif el[-2] == '0':\n",
    "            negs.append(el)\n",
    "\n",
    "    pos_index = int(len(poss) * ratio)\n",
    "    neg_index = int(len(negs) * ratio)\n",
    "\n",
    "    pos_train = poss[:pos_index]\n",
    "    pos_test = poss[pos_index:]\n",
    "    neg_train = negs[:neg_index]\n",
    "    neg_test = negs[neg_index:]\n",
    "    train = pos_train + neg_train\n",
    "    test = pos_test + neg_test\n",
    "    return train, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('whole_set.txt', 'r') as set_file:\n",
    "    set = set_file.readlines()\n",
    "\n",
    "# Generate the trein set and test set\n",
    "train, test = split_sets(set, 0.7)\n",
    "\n",
    "# Generate the two folds \n",
    "fold_1, fold_2 = split_sets(train, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of positives and negatives in each set\n",
    "\n",
    "# Train\n",
    "poss = []\n",
    "negs = []\n",
    "for el in train:\n",
    "        if el[-2] == '1':\n",
    "            poss.append(el)\n",
    "        elif el[-2] == '0':\n",
    "            negs.append(el)\n",
    "print( 'Train -> positives: ', len(poss), 'negatives: ', len(negs)) \n",
    "\n",
    "# Test\n",
    "poss = []\n",
    "negs = []\n",
    "for el in test:\n",
    "        if el[-2] == '1':\n",
    "            poss.append(el)\n",
    "        elif el[-2] == '0':\n",
    "            negs.append(el)\n",
    "print( 'Test -> positives: ', len(poss), 'negatives: ', len(negs)) \n",
    "\n",
    "# Fold 1\n",
    "poss = []\n",
    "negs = []\n",
    "for el in fold_1:\n",
    "        if el[-2] == '1':\n",
    "            poss.append(el)\n",
    "        elif el[-2] == '0':\n",
    "            negs.append(el)\n",
    "print( 'Fold 1 -> positives: ', len(poss), 'negatives: ', len(negs)) \n",
    "\n",
    "# Fold 2\n",
    "poss = []\n",
    "negs = []\n",
    "for el in fold_2:\n",
    "        if el[-2] == '1':\n",
    "            poss.append(el)\n",
    "        elif el[-2] == '0':\n",
    "            negs.append(el)\n",
    "print( 'Fold 2 -> positives: ', len(poss), 'negatives: ', len(negs)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to evaluate the performances of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "from pandas.plotting import table\n",
    "from matplotlib.font_manager import FontProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(predictions):\n",
    "\n",
    "    '''Takes the file with the information about the HMM predictions\n",
    "    and returns a list of lists, containing in each one the id of the \n",
    "    protein tested, its E-value and its label'''\n",
    "\n",
    "    preds=[]\n",
    "    for el in predictions:\n",
    "        fields = el.rstrip().split()\n",
    "        fields[1] = float(fields[1])\n",
    "        fields[2] = int(fields[2])\n",
    "        preds.append(fields)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_confusion_matrix(preds, th=0.5):\n",
    "    pred_lab = []\n",
    "    true_lab = []\n",
    "    false_neg = []\n",
    "    false_pos = []\n",
    "    for values in preds:  \n",
    "        if values[1] <= th:\n",
    "            pred_lab.append(1)\n",
    "        else:\n",
    "            pred_lab.append(0)\n",
    "        true_lab.append(values[2])\n",
    "    \n",
    "    for pos in range(len(true_lab)):   #detect proteins that have been misclassified by the HMM (flase positives and false negatives)\n",
    "        if pred_lab[pos] == 0 and true_lab[pos] == 1:\n",
    "            false_neg.append(preds[pos])\n",
    "        elif pred_lab[pos] == 1 and true_lab[pos] == 0:\n",
    "            false_pos.append(preds[pos])\n",
    "\n",
    "    cm = confusion_matrix(true_lab, pred_lab)\n",
    "    cm = np.flipud(np.fliplr(cm))\n",
    "    return cm, false_neg, false_pos\n",
    "\n",
    "def get_performance_metrics(cm):\n",
    "    tp = cm[0, 0]\n",
    "    tn = cm[1, 1]\n",
    "    fp = cm[1, 0]\n",
    "    fn = cm[0, 1]\n",
    "    \n",
    "    denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    mcc = ((tp * tn) - (fp * fn)) / denominator if denominator != 0 else 0\n",
    "    \n",
    "    accuracy = (tp + tn) / np.sum(cm)\n",
    "    true_pos_rate = tp / (tp + fn)\n",
    "    false_pos_rate = fp / (fp + tn)\n",
    "    \n",
    "    return mcc, accuracy, true_pos_rate, false_pos_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fold 1 -> training set, Fold 2 -> testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = get_data(fold_1)\n",
    "\n",
    "thresholds = (10**-i for i in range(-10, 10))\n",
    "best_performance_1 = float('-inf')\n",
    "best_threshold_1 = None\n",
    "df_1 = pd.DataFrame(columns = [ 'Accuracy', 'MCC', 'TPR', 'FPR'])\n",
    "\n",
    "# Find the E-value that gives the best results on fold 1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    cm_1, _, _ = get_confusion_matrix(data_1, threshold)\n",
    "    accuracy_1, mcc_1, true_pos_rate_1, false_pos_rate_1 = get_performance_metrics(cm_1)\n",
    "    df_1.loc[threshold] = [float(f'{accuracy_1:.3g}'), float(f'{mcc_1:.3g}'), \n",
    "                           float(f'{true_pos_rate_1:.3g}'), float(f'{false_pos_rate_1:.3g}')]\n",
    "\n",
    "    if mcc_1 > best_performance_1:\n",
    "        best_performance_1 = mcc_1\n",
    "        best_threshold_1 = threshold\n",
    "\n",
    "print(best_threshold_1, best_performance_1, cm_1)\n",
    "\n",
    "df_1.index.name = 'E-value'\n",
    "#df_1.title = 'Fold 1 performance metrics'\n",
    "#df_1.to_csv('performances_train_fold_1.csv', sep='\\t')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))  # Set size frame\n",
    "\n",
    "# Hide axes\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Create the table\n",
    "tbl = table(ax, df_1, loc='center', cellLoc='center', colWidths=[0.1] * len(df_1.columns))\n",
    "\n",
    "# Style the table\n",
    "tbl.auto_set_font_size(False)\n",
    "tbl.set_fontsize(8)\n",
    "tbl.scale(1.2, 1.2)\n",
    "font = FontProperties(family = 'Verdana')\n",
    "for (i, j), cell in tbl.get_celld().items():\n",
    "    cell.set_edgecolor('black')\n",
    "    cell.set_linewidth(0.5)\n",
    "    cell.set_text_props(fontproperties = font)\n",
    "    if i == 0:\n",
    "        cell.set_text_props(fontweight='bold', fontsize = 10)\n",
    "        cell.set_facecolor('#D3D3D3')\n",
    "    elif df_1.index[i-1] == best_threshold_1:\n",
    "        cell.set_text_props(fontweight='bold')\n",
    "# Save the table as an image\n",
    "plt.savefig(\"table_1.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the selected best thershold to measure the model performance on the second fold\n",
    "\n",
    "data_2 = get_data(fold_2)\n",
    "cm_test_1, _, _ = get_confusion_matrix(data_2, best_threshold_1)\n",
    "accuracy_test_1, mcc_test_1, true_pos_rate_test_1, false_pos_rate_test_1 = get_performance_metrics(cm_test_1)\n",
    "print(cm_test_1, best_threshold_1, accuracy_test_1, mcc_test_1, true_pos_rate_test_1, false_pos_rate_test_1)\n",
    "df_test_fold_2 = pd.DataFrame(columns = ['Accuracy', 'MCC', 'TPR', 'FPR'])\n",
    "df_test_fold_2.loc[best_threshold_1] = [float(f'{accuracy_test_1:.3g}'), float(f'{mcc_test_1:.3g}'), \n",
    "                           float(f'{true_pos_rate_test_1:.3g}'), float(f'{false_pos_rate_test_1:.3g}')]\n",
    "\n",
    "# Plot the obtained results\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "custom_annotations = [['TRUE POSITIVES', 'FALSE NEGATIVES'], ['FALSE POSITIVES', 'TRUE NEGATIVES']]\n",
    "sns.heatmap(cm_test_1, annot=True, cmap='Blues', fmt='g', xticklabels=['Positive', 'Negative'], yticklabels=['Positive', 'Negative'], cbar = False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix: Fold 2 test set')\n",
    "plt.savefig('confusion_matrix_train_fold_2.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))  # Set size frame\n",
    "\n",
    "# Hide axes\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Create the table\n",
    "tbl_2 = table(ax, df_test_fold_2, loc='center', cellLoc='center', colWidths=[0.1] * len(df_test_fold_2.columns))\n",
    "\n",
    "# Style the table\n",
    "tbl_2.auto_set_font_size(False)\n",
    "tbl_2.set_fontsize(8)\n",
    "tbl_2.scale(1.2, 1.2)\n",
    "font = FontProperties(family = 'Verdana')\n",
    "for (i, j), cell in tbl_2.get_celld().items():\n",
    "    cell.set_edgecolor('black')\n",
    "    cell.set_linewidth(0.5)\n",
    "    cell.set_text_props(fontproperties = font,)\n",
    "    if i == 0:\n",
    "        cell.set_text_props(fontweight='bold', fontsize = 10)\n",
    "        cell.set_facecolor('#D3D3D3')\n",
    "\n",
    "# Save the table as an image\n",
    "plt.savefig(\"table_test_fold_2.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the fold 2 to find the best E-value and use it in the fold 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = (10**-i for i in range(-10, 10))\n",
    "best_performance_2 = float('-inf')\n",
    "best_threshold_2 = None\n",
    "\n",
    "df_2 = pd.DataFrame(columns = ['Accuracy', 'MCC', 'TPR', 'FPR'])\n",
    "\n",
    "#Find the E-value that gives the best results on fold 2\n",
    "\n",
    "for threshold in thresholds:\n",
    "    cm_2, _, _ = get_confusion_matrix(data_2, threshold)\n",
    "    accuracy_2, mcc_2, true_pos_rate_2, false_pos_rate_2 = get_performance_metrics(cm_2)\n",
    "    df_2.loc[threshold] = [float(f'{accuracy_2:.3g}'), float(f'{mcc_2:.3g}'), \n",
    "                           float(f'{true_pos_rate_2:.3g}'), float(f'{false_pos_rate_2:.3g}')]\n",
    "\n",
    "    if mcc_2 > best_performance_2:\n",
    "        best_performance_2 = mcc_2\n",
    "        best_threshold_2 = threshold\n",
    "\n",
    "df_2.index.name = 'E-value'\n",
    "df_2.title = 'Fold 2 performance metrics'\n",
    "#df_2.to_csv('performances_train_fold_2.csv', sep='\\t')\n",
    "\n",
    "print(best_threshold_2, best_performance_2, cm_2, )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))  # Set size frame\n",
    "\n",
    "# Hide axes\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Create the table\n",
    "tbl_3 = table(ax, df_2, loc='center', cellLoc='center', colWidths=[0.1] * len(df_2.columns))\n",
    "\n",
    "# Style the table\n",
    "tbl_3.auto_set_font_size(False)\n",
    "tbl_3.set_fontsize(8)\n",
    "tbl_3.scale(1.2, 1.2)\n",
    "font = FontProperties(family = 'Verdana')\n",
    "for (i, j), cell in tbl_3.get_celld().items():\n",
    "    cell.set_edgecolor('black')\n",
    "    cell.set_linewidth(0.5)\n",
    "    cell.set_text_props(fontproperties = font)\n",
    "    if i == 0:\n",
    "        cell.set_text_props(fontweight='bold', fontsize = 10)\n",
    "        cell.set_facecolor('#D3D3D3')\n",
    "    elif df_1.index[i-1] == best_threshold_2:\n",
    "        cell.set_text_props(fontweight='bold')\n",
    "# Save the table as an image\n",
    "plt.savefig(\"table_3.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the selected best thershold to measure the model performance on the first fold\n",
    "\n",
    "cm_test_2, _, _ = get_confusion_matrix(data_1, best_threshold_2)\n",
    "accuracy_test_2, mcc_test_2, true_pos_rate_test_2, false_pos_rate_test_2 = get_performance_metrics(cm_test_2)\n",
    "print(cm_test_2, best_threshold_2, accuracy_test_2, mcc_test_2, true_pos_rate_test_2, false_pos_rate_test_2)\n",
    "\n",
    "df_test_fold_1 = pd.DataFrame(columns = ['Accuracy', 'MCC', 'TPR', 'FPR'])\n",
    "df_test_fold_1.loc[best_threshold_2] = [float(f'{accuracy_test_2:.3g}'), float(f'{mcc_test_2:.3g}'), \n",
    "                           float(f'{true_pos_rate_test_2:.3g}'), float(f'{false_pos_rate_test_2:.3g}')]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_test_2, annot=True, cmap='Blues', fmt='g', xticklabels=['Positive', 'Negative'], yticklabels=['Positive', 'Negative'], cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix: Fold 1 test set')\n",
    "plt.savefig('confusion_matrix_fold_1_test.png')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))  # Set size frame\n",
    "\n",
    "# Hide axes\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Create the table\n",
    "tbl_4 = table(ax, df_test_fold_1, loc='center', cellLoc='center', colWidths=[0.1] * len(df_test_fold_1.columns))\n",
    "\n",
    "# Style the table\n",
    "tbl_4.auto_set_font_size(False)\n",
    "tbl_4.set_fontsize(8)\n",
    "tbl_4.scale(1.2, 1.2)\n",
    "font = FontProperties(family = 'Verdana')\n",
    "for (i, j), cell in tbl_4.get_celld().items():\n",
    "    cell.set_edgecolor('black')\n",
    "    cell.set_linewidth(0.5)\n",
    "    cell.set_text_props(fontproperties = font,)\n",
    "    if i == 0:\n",
    "        cell.set_text_props(fontweight='bold', fontsize = 10)\n",
    "        cell.set_facecolor('#D3D3D3')\n",
    "\n",
    "# Save the table as an image\n",
    "plt.savefig(\"table_test_fold_1.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make the average between the two selected E-values and use that value to test the generalization of the model through the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_E_value = best_threshold_1 + best_threshold_2 / 2\n",
    "print(average_E_value)\n",
    "\n",
    "test_set_data = get_data(test)\n",
    "cm_test_set, false_neg_test_set, false_pos_test_set = get_confusion_matrix(test_set_data, average_E_value)\n",
    "acc_test_set, mcc_test_set, true_pos_test_set, false_pos_rate_test_set = get_performance_metrics(cm_test_set)\n",
    "print(cm_test_set, acc_test_set, mcc_test_set, true_pos_test_set, false_pos_rate_test_set, false_neg_test_set)\n",
    "\n",
    "df_test_set = pd.DataFrame(columns = ['Accuracy', 'MCC', 'TPR', 'FPR'])\n",
    "df_test_set.loc[average_E_value] = [float(f'{acc_test_set:.3g}'), float(f'{mcc_test_set:.3g}'), \n",
    "                           float(f'{true_pos_test_set:.3g}'), float(f'{false_pos_rate_test_set:.3g}')]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_test_set, annot=True, cmap='Blues', fmt='g', xticklabels=['Positive', 'Negative'], yticklabels=['Positive', 'Negative'], cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix Test Set')\n",
    "plt.savefig('confusion_matrix_test_set.png')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))  # Set size frame\n",
    "\n",
    "# Hide axes\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Create the table\n",
    "tbl_5 = table(ax, df_test_set, loc='center', cellLoc='center', colWidths=[0.1] * len(df_test_set.columns))\n",
    "\n",
    "# Style the table\n",
    "tbl_5.auto_set_font_size(False)\n",
    "tbl_5.set_fontsize(8)\n",
    "tbl_5.scale(1.2, 1.2)\n",
    "font = FontProperties(family = 'Verdana')\n",
    "for (i, j), cell in tbl_5.get_celld().items():\n",
    "    cell.set_edgecolor('black')\n",
    "    cell.set_linewidth(0.5)\n",
    "    cell.set_text_props(fontproperties = font,)\n",
    "    if i == 0:\n",
    "        cell.set_text_props(fontweight='bold', fontsize = 10)\n",
    "        cell.set_facecolor('#D3D3D3')\n",
    "\n",
    "# Save the table as an image\n",
    "plt.savefig(\"table_test_set.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels_test = []\n",
    "predict_labels_test = []\n",
    "for ex in test_set_data:\n",
    "    real_labels_test.append(ex[2])\n",
    "    if ex[1] <= average_E_value:\n",
    "        predict_labels_test.append(1)\n",
    "    else:\n",
    "        predict_labels_test.append(0)\n",
    "\n",
    "false_pos_rate_ROC, true_pos_rate_ROC, _ = roc_curve(real_labels_test, predict_labels_test)\n",
    "roc_auc = auc(false_pos_rate_ROC, true_pos_rate_ROC)\n",
    "print(roc_auc)\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(false_pos_rate_ROC, true_pos_rate_ROC, color='darkorange', lw=lw, label=f'ROC curve HMM (area = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', label = 'Null model')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_curve.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
